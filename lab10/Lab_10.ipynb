{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Final Lab: Classification and Clustering Grab Bag\n",
      "\n",
      "In this final lab, we will introduce two methods of classification called Neural Networks and Support Vector Machines, and an additional method of clustering called Heirarchical Agglomerative Clustering. We hope that introducing these methods will broaden your toolkit for tackling problems both for the final project and in the future. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import math\n",
      "import scipy\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Neural Networks\n",
      "\n",
      "A neural network \"learns\" by solving an optimization problem to choose a set of parameters that minimizes an error function, which is typically a squared error loss. This definition of learning isn't unique to a neural network model. Consider, in the simplest case, a linear model of the form $y = XB$. Given a vector of data  $y \\in \\mathbb{R}^m$, we choose  $B \\in \\mathbb{R}^n$ that minimizes $||y-XB||_2$, where $X$ is an $m$ by $n$ matrix with \"training data\" on the rows.  Solving this least squares minimization problem has a nice well known closed form analytical result: $\\hat{B} = (X^TX)^{-1}X^Ty$. There is, however, a tradeoff between computational ease of finding optimal model parameters and model complexity.  This is evident in the linear case, since the model is extremely easy to fit but has a very simple form. The neural network attempts to find a \"sweet spot\": while the model is highly non-linear, its particular functional form allows for a computationally slick fitting procedure called \"backpropogation\". \n",
      "\n",
      "A \"neural network\" is a function from $f: \\mathbb{R}^m \\rightarrow \\{-1,1\\}$. The input to each neuron is a linear combination of the outputs of each of the neurons in the lower layer. The output of the neuron is a nonlinear threshold applied to its input: it's something that maps the real line to (-1,1) in a 1-1 fashion such that \"most\" of the positive line is mapped pretty close to 1 and and \"most\" of the negative line is mapped pretty close to -1. A common choice is the function  $g(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$.The final output of the signal is the sign of the top neuron. Note that the top layer is constrained to have a single neuron, and we repress applying $g$ to it.\n",
      "\n",
      "The goal of fitting the model is to find a suitable set of  \"weights\", which we can compactly refer to as $w$, for the inputs of each of the neurons. One way to do this is to choose a set of weights that minimize the sum of squared errors for training examples we already have. The intuition here is to choose a model that is \"close\" to the true model, just like we would in a linear regression. However, unlike linear regression, an analytical solution is not feasible because of the ugly threshold functions, so we need to resort to a computational approach like gradient descent. Gradient descent takes steps in the direction of greatest error decrease in the parameter space, hoping to find a (global) minimum. (Recall that from multivariable calculus, the gradient of scalar field points in the direction of the greatest increase of the function, and so walking in the opposite direction points in the direction of greatest decrease repeating the argument with the negative of the function.) \n",
      "\n",
      "We start by initializing the model with some arbitrary set of weights. Feeding forward, given an input $x \\in \\mathbb{R}^m$, we may compute the output as delineated above. Hence, we may calculate the error $E = (y-f(x))^2$, which is the error from the first training example. Now, we compute the gradient of this error function with respect to the weights. \n",
      "\n",
      "The beauty of the method is that computing the gradient of the error is computationally slick and can be done recursively using the chain rule. To see this, we'll introduce some notation. Let $k = 1...L$ indicate layers, assume that $s_{jk}$ is the output of the jth neuron in layer $k$, $x_{jk}$ is the input into neuron $j$ in layer $k$, and $w_{ijk}$ is the weight for the signal input into the $j$th neuron in layer $k$ coming from the ith neuron  in layer $k-1$, so that $x_{jk} = \\sum_{i=1}^{d_{k-1}}w_{ijk}s_{ik-1}$, $s_{jk} = g(x_{jk})$. Note that $d_{k-1}$ stands for the number of neurons in layer $k-1$.  By the chain rule, $$\\frac{\\partial E}{\\partial w_{ijk}} = \\frac{\\partial E}{\\partial s_{jk}}\\frac{\\partial s_{jk}}{\\partial w_{ijk}}$$. Since $s_{jk}$ is linear in the weights, the tricky part is only in computing the former component of the product. We may recursively compute this as \n",
      "\n",
      "$$ \\frac{\\partial E}{\\partial s_{jk}} = \\frac{\\partial E}{\\partial x_{jk}}\\frac{\\partial x_{jk}}{\\partial s_{jk}} = \\sum_{i=1}^{d_{k+1}} \\frac{\\partial E}{\\partial s_{ik+1}}\\frac{\\partial s_{ik+1}}{\\partial x_{jk}}\\frac{\\partial x_{jk}}{\\partial s_{jk}}$$\n",
      "\n",
      "The recursion terminates since $\\frac{\\partial E}{\\partial x_{1L}} = 2(x_{1L}-y)$. We start by computing the gradients from the top layer, and store the gradients as we progress down each layer,so that we need not recompute the gradients. This leads to computational efficiency.  \n",
      "\n",
      "Thus \"learning\" in a neural network  is nothing but switching between computing the error using the training data and updating the weights by calculating the gradient of the error function. In the code block that follows, we write a class for NeuralNetworks, and apply it to performing classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Neural Network Class \n",
      "class Neural_Net:\n",
      "\n",
      "\t#constructor initializes a new neural network with randomly selected weights and pre-specified height, and number of neurons per layer\n",
      "\tdef __init__(self,non,height):\n",
      "\t\t#list to store the number of neurons in each layer of the network\n",
      "\t\tself.num_of_neurons = non\n",
      "\t\t#height of the network\n",
      "\t\tself.L = height\n",
      "\t\t#list to store number of weights in each layer of the network, indexed by layer, output neuron, input neuron\n",
      "\t\tself.weights = numpy.zeros(shape=(10,10,10))\n",
      "\t\t#delta_matrix: stores the gradient that is used in backpropagation\n",
      "\t\tself.deltas = numpy.zeros(shape=(10,10))\n",
      "\t\t#matrix that stores thresholded signals\n",
      "\t\tself.signals = numpy.zeros(shape=(10,10))\n",
      "\t\t#(tunable) learning_rate used in backpropagation\n",
      "\t\tself.learning_rate = .001\n",
      "\t\t#initialize weights to be between -2 and 2\n",
      "\t\tfor i in range(1,self.L+1):\n",
      "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
      "\t\t\t\tfor k in range(self.num_of_neurons[i-1]+1):\n",
      "\t\t\t\t\tself.weights[i][j][k] = random.random()*4-2\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t#forward_pass computes the output of the neural network given an input\n",
      "\tdef forward_pass(self,x):\n",
      "\t\t#(for convenience, we index neurons starting at 1 instead of zero)\n",
      "\t\tself.signals[0][0] = -1\n",
      "\t\tfor i in range(1,self.num_of_neurons[0]+1):\n",
      "\t\t\tself.signals[0][i] = x[i-1]\n",
      "\t\tfor i in range(1,self.L+1):\n",
      "\t\t\tself.signals[i][0] = -1\n",
      "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
      "\t\t\t\tself.signals[i][j] = self.compute_signal(i,j)\n",
      "\t\treturn self.signals[self.L][1]\n",
      "\t\t\t\t\t\n",
      "\t#tune_weights performs the backpropagation algorithm given a training example as input\n",
      "\tdef tune_weights(self,y):\n",
      "\t\tself.deltas[self.L][1] = 2*(self.signals[self.L][1]-y)*(1-math.pow(self.signals[self.L][1],2))\n",
      "\t\tfor i in range(self.L-1,0,-1):\n",
      "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
      "\t\t\t\tself.deltas[i][j] = self.compute_delta(i,j)\n",
      "\t\tfor i in range(1,self.L+1):\n",
      "\t\t\tfor j in range(1,self.num_of_neurons[i]+1):\n",
      "\t\t\t\tfor k in range(self.num_of_neurons[i-1]+1):\n",
      "\t\t\t\t\tself.weights[i][j][k] = self.weights[i][j][k]-self.learning_rate*self.signals[i-1][k]*self.deltas[i][j]\n",
      "\t\n",
      "\t#compute_signal: computes the delta for a given neuron at a given level\n",
      "\tdef compute_signal(self,level,neuron):\n",
      "\t\ts = 0\n",
      "\t\tfor i in range(self.num_of_neurons[level-1]+1):\n",
      "\t\t\ts += self.weights[level][neuron][i]*self.signals[level-1][i]\n",
      "\t\treturn self.g(s)\n",
      "\t\n",
      "\t#compute_delta: computes the signal s for a given neuron at a given level\n",
      "\tdef compute_delta(self,level,neuron):\n",
      "\t\ts = 0\n",
      "\t\tfor j in range(1,self.num_of_neurons[level+1]+1):\n",
      "\t\t\ts += self.weights[level+1][j][neuron]*self.deltas[level+1][j]\n",
      "\t\treturn (1-math.pow(self.signals[level][neuron],2))*s\n",
      "\t\n",
      "\t#soft threshold function\n",
      "\tdef g(self,s):\n",
      "\t\treturn (math.exp(s)-math.exp(-s))/(math.exp(s)+math.exp(-s))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's train a neural network and see how well it performs on the test and training sets epoch by epoch. We will use a mock training and test set with two covariates. We instantiate a neural network with one hidden layer with four neurons, and a learning rate of .001. The learning rate is how much we scale the gradient in \"walking\" the parameter space. \n",
      "\n",
      "To gain some intuition, try to tweak some of these knobs.\n",
      "\n",
      "Note that this will take at least a few minutes to run!\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#read in the train and test dat, assuming csv format\n",
      "training = numpy.genfromtxt('train.csv',delimiter = ',')\n",
      "testing = numpy.genfromtxt('test.csv',delimiter = ',')\n",
      "\n",
      "#specify the number of neurons in each layer\n",
      "num_of_neurons = [2,4,1]\n",
      "#initialize two new neural networks\n",
      "network = Neural_Net(num_of_neurons,2)\n",
      "\n",
      "#store the training error and test error during each epoch\n",
      "training_error = 0\n",
      "test_error = 0\n",
      "\n",
      "#store the training and test error for all epochs\n",
      "train = numpy.zeros(shape = (1000))\n",
      "test = numpy.zeros(shape = (1000))\n",
      "\n",
      "for epoch in range(1000):\n",
      "\ttraining_error = 0\n",
      "\ttest_error = 0\n",
      "\t#compute the test errors\n",
      "\tfor j in range(250):\n",
      "\t\ttest_error = test_error+math.pow(network.forward_pass(testing[j]) - testing[j][2], 2)\n",
      "\t#compute the training errors, SEQUENTIALLY. In other words, we perform backpropagation for *every* example\n",
      "\t#instead of all at once. \n",
      "\tfor i in range(25):\n",
      "\t\ttraining_error = training_error+math.pow(network.forward_pass(training[i])- training[i][2], 2)\n",
      "\t\tnetwork.tune_weights(training[i][2])\t   \n",
      "\ttraining_error = training_error/25\n",
      "\ttest_error = test_error/250\n",
      "\ttrain[epoch] = training_error\n",
      "\ttest[epoch]  = test_error\n",
      "\n",
      "print train\n",
      "print test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.79752294  1.77567585  1.75465133  1.73466188  1.71586161  1.69834244\n",
        "  1.68213692  1.66722633  1.65355148  1.64102453  1.62953988  1.61898346\n",
        "  1.60923973  1.60019679  1.59174947  1.58380118  1.57626456  1.56906154\n",
        "  1.56212282  1.55538721  1.54880077  1.542316    1.53589092  1.52948839\n",
        "  1.52307537  1.51662233  1.5101027   1.50349242  1.4967696   1.48991415\n",
        "  1.48290759  1.4757328   1.46837389  1.46081615  1.4530459   1.44505059\n",
        "  1.43681873  1.42834004  1.41960551  1.41060758  1.4013403   1.39179955\n",
        "  1.38198334  1.37189201  1.36152859  1.35089905  1.34001264  1.32888212\n",
        "  1.31752405  1.30595889  1.29421113  1.28230921  1.2702854   1.25817545\n",
        "  1.24601813  1.23385456  1.22172747  1.20968022  1.19775589  1.18599622\n",
        "  1.1744406   1.1631252   1.15208215  1.14133897  1.13091819  1.12083713\n",
        "  1.11110801  1.10173812  1.09273023  1.08408308  1.07579196  1.0678493\n",
        "  1.06024525  1.05296828  1.04600565  1.03934386  1.03296904  1.02686721\n",
        "  1.02102454  1.01542751  1.01006306  1.00491864  0.99998224  0.99524247\n",
        "  0.9906885   0.9863101   0.98209759  0.97804181  0.97413412  0.97036634\n",
        "  0.96673075  0.96322004  0.95982731  0.956546    0.95336994  0.95029326\n",
        "  0.94731041  0.94441615  0.9416055   0.93887375  0.93621646  0.93362941\n",
        "  0.93110862  0.92865032  0.92625095  0.92390717  0.92161578  0.9193738\n",
        "  0.9171784   0.91502691  0.91291683  0.91084579  0.90881154  0.906812\n",
        "  0.90484518  0.90290922  0.90100236  0.89912296  0.89726946  0.8954404\n",
        "  0.89363441  0.89185018  0.89008651  0.88834225  0.88661633  0.88490773\n",
        "  0.8832155   0.88153874  0.8798766   0.87822831  0.8765931   0.87497027\n",
        "  0.87335916  0.87175915  0.87016964  0.86859008  0.86701995  0.86545875\n",
        "  0.86390602  0.86236131  0.8608242   0.85929431  0.85777125  0.85625468\n",
        "  0.85474426  0.85323967  0.8517406   0.85024678  0.84875793  0.84727378\n",
        "  0.8457941   0.84431865  0.84284719  0.84137952  0.83991543  0.83845472\n",
        "  0.8369972   0.8355427   0.83409104  0.83264206  0.83119558  0.82975147\n",
        "  0.82830957  0.82686974  0.82543184  0.82399574  0.82256131  0.82112842\n",
        "  0.81969697  0.81826682  0.81683787  0.81541001  0.81398314  0.81255715\n",
        "  0.81113194  0.80970743  0.80828351  0.8068601   0.80543711  0.80401446\n",
        "  0.80259206  0.80116984  0.79974772  0.79832563  0.79690349  0.79548124\n",
        "  0.79405881  0.79263614  0.79121315  0.7897898   0.78836603  0.78694178\n",
        "  0.78551699  0.78409163  0.78266563  0.78123895  0.77981154  0.77838338\n",
        "  0.7769544   0.77552458  0.77409388  0.77266226  0.7712297   0.76979616\n",
        "  0.76836161  0.76692602  0.76548938  0.76405166  0.76261284  0.7611729\n",
        "  0.75973182  0.7582896   0.7568462   0.75540163  0.75395587  0.75250892\n",
        "  0.75106077  0.74961141  0.74816084  0.74670906  0.74525607  0.74380188\n",
        "  0.74234648  0.74088987  0.73943208  0.7379731   0.73651293  0.73505161\n",
        "  0.73358912  0.73212549  0.73066074  0.72919487  0.7277279   0.72625985\n",
        "  0.72479075  0.7233206   0.72184943  0.72037727  0.71890413  0.71743003\n",
        "  0.71595501  0.71447909  0.71300229  0.71152464  0.71004616  0.7085669\n",
        "  0.70708687  0.7056061   0.70412463  0.70264248  0.70115968  0.69967628\n",
        "  0.69819229  0.69670775  0.69522269  0.69373716  0.69225116  0.69076476\n",
        "  0.68927796  0.68779082  0.68630336  0.68481562  0.68332762  0.68183942\n",
        "  0.68035103  0.6788625   0.67737385  0.67588513  0.67439637  0.6729076\n",
        "  0.67141885  0.66993016  0.66844156  0.6669531   0.66546479  0.66397668\n",
        "  0.66248879  0.66100117  0.65951384  0.65802684  0.65654019  0.65505394\n",
        "  0.65356811  0.65208274  0.65059786  0.64911349  0.64762968  0.64614644\n",
        "  0.64466382  0.64318184  0.64170054  0.64021993  0.63874006  0.63726095\n",
        "  0.63578263  0.63430512  0.63282847  0.63135269  0.62987782  0.62840388\n",
        "  0.6269309   0.6254589   0.62398792  0.62251799  0.62104911  0.61958134\n",
        "  0.61811468  0.61664917  0.61518483  0.61372169  0.61225977  0.6107991\n",
        "  0.6093397   0.6078816   0.60642482  0.60496939  0.60351533  0.60206267\n",
        "  0.60061142  0.59916162  0.59771328  0.59626644  0.5948211   0.59337731\n",
        "  0.59193508  0.59049443  0.58905539  0.58761798  0.58618222  0.58474814\n",
        "  0.58331575  0.58188509  0.58045618  0.57902903  0.57760367  0.57618012\n",
        "  0.57475841  0.57333856  0.57192059  0.57050452  0.56909038  0.56767818\n",
        "  0.56626796  0.56485973  0.56345352  0.56204935  0.56064723  0.55924721\n",
        "  0.55784928  0.55645349  0.55505986  0.55366839  0.55227913  0.55089208\n",
        "  0.54950728  0.54812475  0.5467445   0.54536657  0.54399097  0.54261773\n",
        "  0.54124687  0.53987842  0.53851239  0.53714881  0.53578771  0.5344291\n",
        "  0.53307302  0.53171947  0.53036849  0.5290201   0.52767432  0.52633118\n",
        "  0.52499069  0.52365289  0.52231779  0.52098542  0.5196558   0.51832895\n",
        "  0.5170049   0.51568366  0.51436527  0.51304975  0.51173711  0.51042738\n",
        "  0.50912058  0.50781674  0.50651588  0.50521801  0.50392317  0.50263137\n",
        "  0.50134264  0.50005699  0.49877446  0.49749505  0.4962188   0.49494572\n",
        "  0.49367583  0.49240916  0.49114573  0.48988555  0.48862865  0.48737504\n",
        "  0.48612475  0.4848778   0.4836342   0.48239398  0.48115715  0.47992374\n",
        "  0.47869375  0.47746721  0.47624414  0.47502456  0.47380847  0.47259591\n",
        "  0.47138688  0.4701814   0.46897949  0.46778116  0.46658643  0.46539532\n",
        "  0.46420783  0.46302399  0.4618438   0.46066729  0.45949445  0.45832532\n",
        "  0.45715989  0.45599819  0.45484021  0.45368599  0.45253552  0.45138881\n",
        "  0.45024588  0.44910674  0.44797139  0.44683985  0.44571212  0.44458821\n",
        "  0.44346813  0.44235188  0.44123948  0.44013092  0.43902622  0.43792539\n",
        "  0.43682841  0.43573531  0.43464609  0.43356074  0.43247927  0.43140169\n",
        "  0.43032799  0.42925818  0.42819227  0.42713025  0.42607212  0.42501788\n",
        "  0.42396753  0.42292108  0.42187852  0.42083985  0.41980507  0.41877418\n",
        "  0.41774716  0.41672403  0.41570478  0.41468939  0.41367788  0.41267023\n",
        "  0.41166644  0.4106665   0.40967041  0.40867816  0.40768974  0.40670516\n",
        "  0.40572439  0.40474743  0.40377428  0.40280492  0.40183935  0.40087755\n",
        "  0.39991952  0.39896525  0.39801472  0.39706793  0.39612487  0.39518552\n",
        "  0.39424987  0.3933179   0.39238962  0.391465    0.39054403  0.3896267\n",
        "  0.38871299  0.3878029   0.3868964   0.38599348  0.38509413  0.38419833\n",
        "  0.38330607  0.38241732  0.38153209  0.38065034  0.37977206  0.37889724\n",
        "  0.37802586  0.3771579   0.37629334  0.37543218  0.37457438  0.37371993\n",
        "  0.37286881  0.37202101  0.37117651  0.37033528  0.36949731  0.36866258\n",
        "  0.36783106  0.36700275  0.36617762  0.36535565  0.36453681  0.3637211\n",
        "  0.36290849  0.36209895  0.36129247  0.36048903  0.3596886   0.35889117\n",
        "  0.35809672  0.35730521  0.35651664  0.35573097  0.35494819  0.35416827\n",
        "  0.3533912   0.35261695  0.3518455   0.35107682  0.3503109   0.34954771\n",
        "  0.34878722  0.34802943  0.34727429  0.3465218   0.34577192  0.34502464\n",
        "  0.34427992  0.34353776  0.34279812  0.34206098  0.34132632  0.34059411\n",
        "  0.33986433  0.33913696  0.33841198  0.33768935  0.33696906  0.33625109\n",
        "  0.3355354   0.33482198  0.3341108   0.33340184  0.33269507  0.33199047\n",
        "  0.33128802  0.33058769  0.32988945  0.32919329  0.32849919  0.3278071\n",
        "  0.32711702  0.32642892  0.32574278  0.32505856  0.32437625  0.32369582\n",
        "  0.32301726  0.32234052  0.3216656   0.32099246  0.32032109  0.31965146\n",
        "  0.31898354  0.31831732  0.31765276  0.31698985  0.31632856  0.31566886\n",
        "  0.31501074  0.31435417  0.31369913  0.31304559  0.31239354  0.31174294\n",
        "  0.31109377  0.31044601  0.30979965  0.30915465  0.30851099  0.30786865\n",
        "  0.3072276   0.30658784  0.30594932  0.30531204  0.30467596  0.30404106\n",
        "  0.30340734  0.30277475  0.30214328  0.30151291  0.30088362  0.30025539\n",
        "  0.29962819  0.29900201  0.29837681  0.29775259  0.29712933  0.29650699\n",
        "  0.29588557  0.29526504  0.29464538  0.29402658  0.29340861  0.29279145\n",
        "  0.29217509  0.29155951  0.29094469  0.29033061  0.28971725  0.2891046\n",
        "  0.28849264  0.28788135  0.28727072  0.28666073  0.28605136  0.2854426\n",
        "  0.28483443  0.28422683  0.2836198   0.28301332  0.28240738  0.28180195\n",
        "  0.28119703  0.28059261  0.27998867  0.27938519  0.27878218  0.27817962\n",
        "  0.27757749  0.27697579  0.2763745   0.27577362  0.27517314  0.27457305\n",
        "  0.27397334  0.273374    0.27277504  0.27217643  0.27157818  0.27098027\n",
        "  0.27038271  0.26978549  0.26918861  0.26859205  0.26799583  0.26739994\n",
        "  0.26680437  0.26620913  0.26561421  0.26501961  0.26442534  0.2638314\n",
        "  0.26323778  0.2626445   0.26205155  0.26145893  0.26086666  0.26027474\n",
        "  0.25968317  0.25909195  0.2585011   0.25791062  0.25732053  0.25673082\n",
        "  0.25614151  0.2555526   0.25496412  0.25437606  0.25378844  0.25320127\n",
        "  0.25261457  0.25202835  0.25144261  0.25085738  0.25027267  0.2496885\n",
        "  0.24910488  0.24852182  0.24793935  0.24735747  0.24677622  0.2461956\n",
        "  0.24561564  0.24503635  0.24445776  0.24387987  0.24330272  0.24272633\n",
        "  0.2421507   0.24157587  0.24100186  0.24042868  0.23985637  0.23928493\n",
        "  0.2387144   0.23814479  0.23757614  0.23700845  0.23644176  0.23587608\n",
        "  0.23531144  0.23474787  0.23418538  0.233624    0.23306376  0.23250467\n",
        "  0.23194675  0.23139004  0.23083456  0.23028032  0.22972735  0.22917567\n",
        "  0.22862531  0.22807629  0.22752862  0.22698234  0.22643746  0.225894\n",
        "  0.22535199  0.22481144  0.22427239  0.22373483  0.22319881  0.22266433\n",
        "  0.22213142  0.22160009  0.22107037  0.22054227  0.2200158   0.219491\n",
        "  0.21896786  0.21844642  0.21792668  0.21740867  0.21689239  0.21637787\n",
        "  0.21586511  0.21535414  0.21484496  0.21433758  0.21383203  0.21332831\n",
        "  0.21282643  0.21232641  0.21182826  0.21133198  0.21083759  0.21034509\n",
        "  0.2098545   0.20936583  0.20887908  0.20839426  0.20791137  0.20743043\n",
        "  0.20695144  0.20647441  0.20599934  0.20552623  0.2050551   0.20458595\n",
        "  0.20411877  0.20365358  0.20319038  0.20272917  0.20226995  0.20181272\n",
        "  0.20135749  0.20090426  0.20045303  0.2000038   0.19955656  0.19911133\n",
        "  0.1986681   0.19822686  0.19778763  0.19735039  0.19691514  0.19648189\n",
        "  0.19605063  0.19562136  0.19519407  0.19476877  0.19434545  0.1939241\n",
        "  0.19350473  0.19308733  0.1926719   0.19225842  0.1918469   0.19143734\n",
        "  0.19102972  0.19062405  0.19022031  0.1898185   0.18941862  0.18902066\n",
        "  0.18862461  0.18823047  0.18783823  0.18744789  0.18705944  0.18667287\n",
        "  0.18628817  0.18590535  0.18552439  0.18514528  0.18476801  0.18439259\n",
        "  0.184019    0.18364724  0.18327729  0.18290915  0.18254282  0.18217828\n",
        "  0.18181552  0.18145455  0.18109534  0.18073789  0.1803822   0.18002825\n",
        "  0.17967604  0.17932556  0.1789768   0.17862975  0.17828441  0.17794075\n",
        "  0.17759879  0.1772585   0.17691988  0.17658292  0.17624761  0.17591395\n",
        "  0.17558191  0.17525151  0.17492272  0.17459553  0.17426995  0.17394595\n",
        "  0.17362354  0.1733027   0.17298342  0.1726657   0.17234952  0.17203488\n",
        "  0.17172176  0.17141017  0.17110009  0.17079151  0.17048442  0.17017881\n",
        "  0.16987469  0.16957202  0.16927082  0.16897106  0.16867275  0.16837587\n",
        "  0.16808041  0.16778636  0.16749372  0.16720248  0.16691262  0.16662415\n",
        "  0.16633705  0.16605131  0.16576693  0.16548389  0.16520219  0.16492182\n",
        "  0.16464277  0.16436504  0.16408861  0.16381347  0.16353963  0.16326706\n",
        "  0.16299576  0.16272573  0.16245696  0.16218943  0.16192315  0.16165809\n",
        "  0.16139426  0.16113165  0.16087024  0.16061003  0.16035102  0.16009319\n",
        "  0.15983654  0.15958106  0.15932674  0.15907358  0.15882156  0.15857068\n",
        "  0.15832094  0.15807232  0.15782481  0.15757842  0.15733313  0.15708894\n",
        "  0.15684583  0.15660381  0.15636286  0.15612298  0.15588416  0.15564639\n",
        "  0.15540967  0.15517398  0.15493933  0.15470571  0.15447311  0.15424152\n",
        "  0.15401093  0.15378135  0.15355275  0.15332515  0.15309852  0.15287287\n",
        "  0.15264818  0.15242445  0.15220168  0.15197986  0.15175898  0.15153903\n",
        "  0.15132002  0.15110192  0.15088475  0.15066849  0.15045313  0.15023867\n",
        "  0.1500251   0.14981243  0.14960063  0.14938971  0.14917966  0.14897048\n",
        "  0.14876215  0.14855468  0.14834806  0.14814228  0.14793734  0.14773323\n",
        "  0.14752994  0.14732748  0.14712583  0.14692499  0.14672495  0.14652572\n",
        "  0.14632728  0.14612963  0.14593276  0.14573668  0.14554137  0.14534682\n",
        "  0.14515305  0.14496003  0.14476776  0.14457625  0.14438548  0.14419545\n",
        "  0.14400616  0.1438176   0.14362976  0.14344265  0.14325625  0.14307057\n",
        "  0.14288559  0.14270132  0.14251774  0.14233486  0.14215267  0.14197117\n",
        "  0.14179035  0.1416102   0.14143073  0.14125193  0.14107379  0.14089632\n",
        "  0.1407195   0.14054333  0.14036781  0.14019293]\n",
        "[ 1.87492126  1.86630143  1.85783549  1.84956297  1.84150797  1.83367975\n",
        "  1.8260746   1.8186785   1.81146995  1.80442261  1.79750753  1.7906949\n",
        "  1.78395525  1.77726022  1.77058304  1.76389872  1.75718408  1.75041768\n",
        "  1.74357967  1.73665164  1.72961644  1.72245796  1.71516104  1.70771126\n",
        "  1.70009488  1.69229865  1.68430982  1.67611602  1.66770521  1.65906573\n",
        "  1.65018619  1.64105561  1.63166336  1.6219993   1.61205385  1.60181809\n",
        "  1.59128394  1.58044431  1.56929333  1.55782658  1.54604135  1.53393695\n",
        "  1.52151502  1.50877988  1.49573889  1.48240278  1.468786    1.45490698\n",
        "  1.44078844  1.42645747  1.41194563  1.39728883  1.38252712  1.36770422\n",
        "  1.35286695  1.33806443  1.32334713  1.30876578  1.29437022  1.28020819\n",
        "  1.26632419  1.25275846  1.23954609  1.22671638  1.21429248  1.20229122\n",
        "  1.19072328  1.1795935   1.16890145  1.15864207  1.14880642  1.13938247\n",
        "  1.1303558   1.12171036  1.11342905  1.10549425  1.0978883   1.09059381\n",
        "  1.08359391  1.07687245  1.07041408  1.06420438  1.05822977  1.05247761\n",
        "  1.04693606  1.04159413  1.03644154  1.03146871  1.02666669  1.02202707\n",
        "  1.01754199  1.01320402  1.00900617  1.00494182  1.00100472  0.9971889\n",
        "  0.99348872  0.98989879  0.98641398  0.98302942  0.97974042  0.97654255\n",
        "  0.97343156  0.97040339  0.96745417  0.96458022  0.96177801  0.95904419\n",
        "  0.95637555  0.95376905  0.95122179  0.94873101  0.94629406  0.94390847\n",
        "  0.94157183  0.9392819   0.93703653  0.93483367  0.93267138  0.93054783\n",
        "  0.92846127  0.92641003  0.92439255  0.92240732  0.92045292  0.91852801\n",
        "  0.91663131  0.9147616   0.91291772  0.91109858  0.90930314  0.9075304\n",
        "  0.90577941  0.90404929  0.90233918  0.90064826  0.89897576  0.89732094\n",
        "  0.89568309  0.89406155  0.89245566  0.89086483  0.88928845  0.88772597\n",
        "  0.88617684  0.88464056  0.88311663  0.88160457  0.88010393  0.87861427\n",
        "  0.87713517  0.87566622  0.87420703  0.87275722  0.87131644  0.86988433\n",
        "  0.86846055  0.86704477  0.86563669  0.86423598  0.86284235  0.86145552\n",
        "  0.86007521  0.85870115  0.85733307  0.85597072  0.85461385  0.85326223\n",
        "  0.85191561  0.85057377  0.8492365   0.84790357  0.84657477  0.84524991\n",
        "  0.84392879  0.84261122  0.841297    0.83998595  0.83867791  0.83737269\n",
        "  0.83607013  0.83477006  0.83347233  0.83217679  0.83088328  0.82959166\n",
        "  0.82830179  0.82701353  0.82572674  0.8244413   0.82315709  0.82187398\n",
        "  0.82059185  0.81931059  0.81803009  0.81675025  0.81547096  0.81419212\n",
        "  0.81291364  0.81163543  0.81035739  0.80907944  0.8078015   0.80652349\n",
        "  0.80524534  0.80396697  0.80268832  0.80140932  0.8001299   0.79885001\n",
        "  0.79756959  0.79628859  0.79500696  0.79372465  0.79244161  0.79115781\n",
        "  0.7898732   0.78858776  0.78730143  0.7860142   0.78472604  0.78343692\n",
        "  0.78214681  0.7808557   0.77956357  0.77827039  0.77697617  0.77568089\n",
        "  0.77438453  0.77308709  0.77178857  0.77048896  0.76918826  0.76788648\n",
        "  0.76658361  0.76527966  0.76397464  0.76266855  0.76136142  0.76005324\n",
        "  0.75874403  0.7574338   0.75612258  0.75481038  0.75349722  0.75218312\n",
        "  0.7508681   0.74955218  0.74823539  0.74691776  0.74559931  0.74428007\n",
        "  0.74296006  0.74163933  0.74031788  0.73899577  0.73767302  0.73634966\n",
        "  0.73502572  0.73370125  0.73237627  0.73105082  0.72972493  0.72839865\n",
        "  0.72707201  0.72574505  0.7244178   0.7230903   0.72176259  0.72043472\n",
        "  0.71910671  0.7177786   0.71645045  0.71512228  0.71379413  0.71246605\n",
        "  0.71113807  0.70981023  0.70848258  0.70715515  0.70582798  0.70450112\n",
        "  0.70317459  0.70184845  0.70052272  0.69919745  0.69787267  0.69654843\n",
        "  0.69522476  0.69390171  0.6925793   0.69125757  0.68993657  0.68861633\n",
        "  0.68729689  0.68597827  0.68466052  0.68334368  0.68202777  0.68071284\n",
        "  0.67939891  0.67808602  0.6767742   0.6754635   0.67415393  0.67284553\n",
        "  0.67153834  0.67023238  0.66892768  0.66762428  0.66632221  0.66502149\n",
        "  0.66372216  0.66242423  0.66112775  0.65983274  0.65853922  0.65724722\n",
        "  0.65595678  0.6546679   0.65338063  0.65209498  0.65081097  0.64952865\n",
        "  0.64824801  0.6469691   0.64569193  0.64441652  0.64314289  0.64187108\n",
        "  0.64060109  0.63933295  0.63806668  0.63680229  0.63553982  0.63427926\n",
        "  0.63302066  0.63176401  0.63050934  0.62925667  0.62800601  0.62675738\n",
        "  0.62551079  0.62426627  0.62302382  0.62178346  0.6205452   0.61930906\n",
        "  0.61807506  0.6168432   0.6156135   0.61438597  0.61316062  0.61193747\n",
        "  0.61071652  0.6094978   0.6082813   0.60706704  0.60585503  0.60464528\n",
        "  0.60343779  0.60223259  0.60102968  0.59982906  0.59863074  0.59743474\n",
        "  0.59624106  0.59504971  0.59386069  0.59267402  0.59148969  0.59030773\n",
        "  0.58912812  0.58795088  0.58677602  0.58560354  0.58443344  0.58326574\n",
        "  0.58210042  0.58093751  0.579777    0.5786189   0.57746321  0.57630994\n",
        "  0.57515908  0.57401065  0.57286465  0.57172107  0.57057992  0.5694412\n",
        "  0.56830492  0.56717108  0.56603967  0.56491071  0.56378419  0.56266011\n",
        "  0.56153847  0.56041928  0.55930254  0.55818824  0.55707638  0.55596697\n",
        "  0.55486001  0.5537555   0.55265342  0.5515538   0.55045662  0.54936188\n",
        "  0.54826958  0.54717972  0.5460923   0.54500732  0.54392477  0.54284465\n",
        "  0.54176697  0.54069172  0.53961889  0.53854849  0.5374805   0.53641494\n",
        "  0.53535179  0.53429105  0.53323272  0.5321768   0.53112328  0.53007216\n",
        "  0.52902342  0.52797708  0.52693313  0.52589156  0.52485236  0.52381553\n",
        "  0.52278108  0.52174898  0.52071925  0.51969186  0.51866683  0.51764413\n",
        "  0.51662378  0.51560575  0.51459005  0.51357667  0.5125656   0.51155684\n",
        "  0.51055038  0.50954622  0.50854434  0.50754475  0.50654744  0.50555239\n",
        "  0.50455961  0.50356908  0.5025808   0.50159476  0.50061096  0.49962938\n",
        "  0.49865003  0.49767288  0.49669795  0.49572521  0.49475465  0.49378629\n",
        "  0.49282009  0.49185606  0.49089419  0.48993447  0.4889769   0.48802145\n",
        "  0.48706814  0.48611694  0.48516785  0.48422086  0.48327596  0.48233315\n",
        "  0.48139241  0.48045374  0.47951712  0.47858256  0.47765004  0.47671954\n",
        "  0.47579107  0.47486462  0.47394017  0.47301771  0.47209724  0.47117875\n",
        "  0.47026223  0.46934766  0.46843505  0.46752438  0.46661564  0.46570882\n",
        "  0.46480392  0.46390092  0.46299981  0.4621006   0.46120325  0.46030778\n",
        "  0.45941416  0.45852239  0.45763246  0.45674436  0.45585808  0.45497361\n",
        "  0.45409095  0.45321007  0.45233098  0.45145367  0.45057812  0.44970432\n",
        "  0.44883227  0.44796196  0.44709337  0.44622651  0.44536135  0.44449789\n",
        "  0.44363613  0.44277604  0.44191763  0.44106089  0.4402058   0.43935235\n",
        "  0.43850054  0.43765036  0.4368018   0.43595485  0.43510951  0.43426575\n",
        "  0.43342358  0.43258299  0.43174396  0.4309065   0.43007058  0.42923621\n",
        "  0.42840336  0.42757205  0.42674225  0.42591396  0.42508718  0.42426188\n",
        "  0.42343808  0.42261575  0.42179488  0.42097549  0.42015754  0.41934105\n",
        "  0.41852599  0.41771236  0.41690016  0.41608938  0.41528001  0.41447204\n",
        "  0.41366547  0.41286029  0.41205649  0.41125407  0.41045302  0.40965333\n",
        "  0.408855    0.40805802  0.40726239  0.40646809  0.40567513  0.4048835\n",
        "  0.40409319  0.4033042   0.40251651  0.40173014  0.40094506  0.40016128\n",
        "  0.39937879  0.39859759  0.39781767  0.39703903  0.39626166  0.39548555\n",
        "  0.39471072  0.39393714  0.39316482  0.39239376  0.39162394  0.39085537\n",
        "  0.39008805  0.38932196  0.38855712  0.3877935   0.38703112  0.38626998\n",
        "  0.38551006  0.38475136  0.38399389  0.38323764  0.38248262  0.38172881\n",
        "  0.38097622  0.38022485  0.3794747   0.37872576  0.37797804  0.37723153\n",
        "  0.37648624  0.37574217  0.37499931  0.37425767  0.37351725  0.37277804\n",
        "  0.37204005  0.37130329  0.37056775  0.36983343  0.36910034  0.36836847\n",
        "  0.36763784  0.36690844  0.36618028  0.36545335  0.36472767  0.36400324\n",
        "  0.36328005  0.36255811  0.36183744  0.36111802  0.36039988  0.359683\n",
        "  0.3589674   0.35825308  0.35754004  0.3568283   0.35611786  0.35540872\n",
        "  0.35470089  0.35399437  0.35328918  0.35258533  0.35188281  0.35118163\n",
        "  0.35048181  0.34978335  0.34908625  0.34839054  0.34769621  0.34700327\n",
        "  0.34631174  0.34562163  0.34493293  0.34424567  0.34355984  0.34287547\n",
        "  0.34219256  0.34151113  0.34083117  0.34015272  0.33947576  0.33880033\n",
        "  0.33812642  0.33745405  0.33678323  0.33611398  0.3354463   0.33478021\n",
        "  0.33411572  0.33345285  0.3327916   0.332132    0.33147404  0.33081776\n",
        "  0.33016315  0.32951024  0.32885903  0.32820955  0.32756181  0.32691581\n",
        "  0.32627158  0.32562913  0.32498847  0.32434962  0.3237126   0.32307741\n",
        "  0.32244408  0.32181261  0.32118303  0.32055535  0.31992958  0.31930574\n",
        "  0.31868385  0.31806391  0.31744595  0.31682998  0.31621602  0.31560408\n",
        "  0.31499417  0.31438632  0.31378053  0.31317682  0.31257521  0.31197572\n",
        "  0.31137835  0.31078312  0.31019005  0.30959915  0.30901044  0.30842393\n",
        "  0.30783963  0.30725756  0.30667773  0.30610016  0.30552487  0.30495185\n",
        "  0.30438113  0.30381273  0.30324664  0.3026829   0.3021215   0.30156246\n",
        "  0.30100579  0.3004515   0.29989961  0.29935013  0.29880306  0.29825843\n",
        "  0.29771623  0.29717647  0.29663917  0.29610434  0.29557199  0.29504212\n",
        "  0.29451474  0.29398986  0.29346749  0.29294763  0.29243029  0.29191548\n",
        "  0.29140321  0.29089347  0.29038628  0.28988164  0.28937955  0.28888002\n",
        "  0.28838306  0.28788866  0.28739682  0.28690756  0.28642087  0.28593676\n",
        "  0.28545522  0.28497626  0.28449988  0.28402607  0.28355484  0.28308619\n",
        "  0.28262012  0.28215662  0.2816957   0.28123735  0.28078156  0.28032835\n",
        "  0.27987769  0.2794296   0.27898406  0.27854107  0.27810064  0.27766274\n",
        "  0.27722738  0.27679455  0.27636425  0.27593647  0.2755112   0.27508844\n",
        "  0.27466818  0.27425041  0.27383512  0.27342232  0.27301198  0.2726041\n",
        "  0.27219868  0.27179569  0.27139515  0.27099703  0.27060132  0.27020802\n",
        "  0.26981711  0.2694286   0.26904245  0.26865868  0.26827725  0.26789818\n",
        "  0.26752143  0.26714701  0.26677489  0.26640508  0.26603755  0.2656723\n",
        "  0.26530931  0.26494858  0.26459008  0.26423381  0.26387976  0.26352791\n",
        "  0.26317825  0.26283077  0.26248546  0.2621423   0.26180128  0.26146238\n",
        "  0.26112561  0.26079093  0.26045834  0.26012783  0.25979938  0.25947298\n",
        "  0.25914862  0.25882628  0.25850595  0.25818761  0.25787126  0.25755688\n",
        "  0.25724446  0.25693399  0.25662544  0.25631881  0.25601409  0.25571125\n",
        "  0.2554103   0.2551112   0.25481396  0.25451856  0.25422498  0.25393322\n",
        "  0.25364325  0.25335507  0.25306866  0.25278401  0.25250111  0.25221994\n",
        "  0.25194049  0.25166274  0.2513867   0.25111233  0.25083963  0.25056859\n",
        "  0.25029919  0.25003142  0.24976527  0.24950072  0.24923777  0.24897639\n",
        "  0.24871659  0.24845833  0.24820163  0.24794645  0.24769279  0.24744063\n",
        "  0.24718997  0.24694079  0.24669309  0.24644683  0.24620203  0.24595866\n",
        "  0.24571671  0.24547617  0.24523703  0.24499927  0.24476289  0.24452788\n",
        "  0.24429422  0.2440619   0.24383091  0.24360123  0.24337287  0.2431458\n",
        "  0.24292001  0.24269551  0.24247226  0.24225026  0.24202951  0.24180999\n",
        "  0.24159169  0.2413746   0.24115871  0.240944    0.24073048  0.24051812\n",
        "  0.24030692  0.24009687  0.23988795  0.23968017  0.2394735   0.23926793\n",
        "  0.23906347  0.23886009  0.23865779  0.23845656  0.23825639  0.23805727\n",
        "  0.23785919  0.23766214  0.23746611  0.23727109  0.23707708  0.23688406\n",
        "  0.23669202  0.23650096  0.23631087  0.23612173  0.23593355  0.2357463\n",
        "  0.23555999  0.2353746   0.23519013  0.23500656  0.23482389  0.23464212\n",
        "  0.23446122  0.2342812   0.23410204  0.23392374  0.23374629  0.23356968\n",
        "  0.2333939   0.23321896  0.23304482  0.23287151  0.23269899  0.23252727\n",
        "  0.23235633  0.23218618  0.2320168   0.23184819  0.23168034  0.23151324\n",
        "  0.23134688  0.23118126  0.23101638  0.23085221  0.23068877  0.23052603\n",
        "  0.230364    0.23020266  0.23004201  0.22988205  0.22972277  0.22956416\n",
        "  0.22940621  0.22924892  0.22909228  0.22893629  0.22878093  0.22862621\n",
        "  0.22847212  0.22831865  0.2281658   0.22801355  0.22786191  0.22771087\n",
        "  0.22756042  0.22741056  0.22726128  0.22711257  0.22696443  0.22681686\n",
        "  0.22666985  0.22652339  0.22637749  0.22623212  0.2260873   0.225943\n",
        "  0.22579924  0.225656    0.22551327  0.22537106  0.22522936  0.22508816\n",
        "  0.22494746  0.22480726  0.22466754  0.22452831  0.22438956  0.22425128\n",
        "  0.22411348  0.22397614  0.22383926  0.22370284  0.22356687  0.22343136\n",
        "  0.22329628  0.22316165  0.22302745  0.22289369  0.22276035  0.22262744\n",
        "  0.22249495  0.22236287  0.22223121  0.22209995  0.2219691   0.22183865\n",
        "  0.2217086   0.22157893  0.22144966  0.22132077]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Support Vector Machines\n",
      "\n",
      "Assume your data set $D$ is = ${(x_1,y_1),...(x_n,y_n)}$, where $x_i \\in \\mathbb{R}^k$ and $y_i \\in \\{1,-1\\}$. A support vector machine attempts to find a plane in $R^n$, that is, a \"hyperplane\", that seperates the data of either class. While there may exist an infinite number of planes that achieve this goal, intuitively we want which maximizes the distance between the seperating hyperplane and the closest vectors, termed the \"support vectors\". The intuition is that this will allow the model to generalize well. It turns out this condition can be codified as a quadratic programming (optimization) problem. \n",
      "\n",
      "Denote an arbitrary hyperplane in $R^k$ as $w^tx+b = 0$. The support vector machine is the solution to the constrained optimization problem:\n",
      "$$minimize$$ $$w^tw$$\n",
      "Subject to constraint:\n",
      "$$(w^tx_i)+b \\geq 1, y_i = 1$$\n",
      "$$(w^tx_i)+b \\leq -1, y_i = -1$$\n",
      "\n",
      "Note that this method is useful only in the case that the data appear to be linearly seperable to begin with. In the case they are not, we apply a \"Kernel\" function to the data which warps the points with the hope that they become linearly seperable. There are a variety of kernels that one could utilize to achieve this goal, and sci-kit learn supports a number of them. In what follows, we implement scikit learn's implementation of the SVM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Support Vector Machine Code Forthcoming..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Hierarchical Agglomerative Clustering\n",
      "\n",
      "The final method we will cover today is hierarchical agglomerative clustering. This is a nice contrast to K-Means which was covered in class becuase one does not need to specify K, the number of clusters, in advance. Interesting applications include processing microarray data in functional genomics and lightcurves in astronomy. Given a data set $D = {x_1,...x_m}$, heirachical clustering proceeds by computing the matrix of pearson similiarities (analgously to homework 4). One then builds a binary tree utilizing this matrix with an iterative procedure. Specifically, the leaves of the tree are the initial $m$ data points. At each step of the iteration, we link the two closest nodes, where the distance between nodes $A$ and $B$ is given by\n",
      "\n",
      "$$D(A,B) = min(s(a,b): a \\in A, b \\in B)$$. \n",
      "\n",
      "where s(.,.) is the familiar pearson similarity from homework 4. Finally, to extract the clusters from this tree, we want to select nodes from which the data is \"highly correlated\". One way to determine this is to check if these pairwise distances within the node are normally distributed, which one can check with the Anderson-Darling test of normality.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Heirarchical Clustering Code Forthcoming..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#References\n",
      "\n",
      "1. \"Learning From Data\" by Yaser S. Abu-Mostafa et al.\n",
      "2. \"Cluster Analysis and Display of Genome-Wide Expression Patterns\" by Eisen at al.\n",
      "3. \"De-Trending Time Series for Astronomical Variability Surveys\" by Kim et al."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}