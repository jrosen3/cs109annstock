<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>A Neural Network Application: Predicting the Stock Market</title>
	<!-- Import jQuery -->
   <script src="js/jquery-1.10.2.min.js"></script>
   <!-- Import Bootstrap -->
  <script src="js/bootstrap.js"></script>
  <script src="js/bootstrap-select.min.js"></script>
  <script src="js/jquery.prettyPhoto.js"></script>  
  
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css">
  <link rel="stylesheet" type="text/css" href="css/bootstrap-select.min.css">
  <link rel="stylesheet" type="text/css" href="css/prettyPhoto.css">
  <!-- Import D3.js -->
  <script src="js/d3.v3.min.js"></script>
  <script src="js/d3.csv.js"></script>
  
  <!-- Import custom libraries -->
  <script src="js/pgfunc.js"></script>
  <script src="js/iris.js"></script>
  <script src="js/sets.js"></script>
  
  <!-- Import fonts -->
<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700' rel='stylesheet' type='text/css'>
<link href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

  <!-- Import custom styles -->
  <link rel="stylesheet" type="text/css" href="css/main.css">
  <link rel="stylesheet" type="text/css" href="css/iris.css">
    
</head>

<body data-spy="scroll" data-target="#ncont" data-offest="100">
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
      <div class="container">
        <h1 class="navbar-header-text">A Neural Network Application: Predicting the Stock Market</h1>
        <div id="ncont">
        <ul class="nav">
            <li class="active"><a href="#introduction">Introduction</a></li>
            <li><a href="#data">Data</a></li>
            <li><a href="#model">Model</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#applications">Applications</a></li>
            <li><a href="#conclusions">Conclusions</a></li>
           </ul>
        </div>
      </div>
</div>
<div class="container" id="introduction">
    <div id="mainvis">
    </div>
    <div class="resultpanel">
    <center><div class="btn btn-primary" id="run_nn">Run Neural Network </div></center>
    <center id = "resultnote" style="display:none;" ><p style="color:#aaa;margin:0px;">* Edge widths are proportional to the actual weights trained after 500 epochs.  </p><p style="color:#aaa;margin:0px;">Blue edges represent positive weights, red represents negative weights.</p></center>
    <h3>Results</h3>
    <p>Based on a Artificial Neural Network (ANN) trained using backpropagation and data from 1999 to 2012, the week-over-week stock market can be predicted to rise or fall with 63.64% accuracy. This prediction is more accurate for increases (79.37%) than for decreases (36.11%).</p>
     <h3>About</h3>
     We constructed a multilayer  perceptron (MLP) feed-forward neural network that used backprogagation to train the weights of the network to predict the direction of the stock market. In the process, we explored a wide variety of possibilities both for the data and neural net parameters.
    </div>
</div>
	<section class ="rowaccent">
    <div class="accentcontainer">
    <center><h2>Background & Motivation</h2></center>
    <p>Although predicting the stock market is a hot area of research, previous approaches have had their shortcomings--they depend on intuition for the selection of input variables because neural networks require a massive amount of computing power. In this project, we aim to improve on other neural network algorithms by adding in data regressions to help us select our input variables. Put simply: we let the data tell us which variables are worth putting in our network. While our work may not be at the scale or scope of larger commercial applications, it does accomplish an augmented application of ANNs for financial data and an advancement in network pruning, a topic of interest in related  scientific literature about financial forecasting with ANNs.</p>
 
   <p>For this project, we wanted to push ourselves. First, designing a neural network has been called an art as well as a science, and we thought it would be interesting to play around with numerous implementations. Second, we wanted to expand the traditional dataset used for financial predictions. With access to Quandl, we could choose from far more variables than other researchers have attempted with this method. Third, we wanted to produce something relevant and applicable, at the end we will be able to know if our work paid off. We can see if we beat the market. </p>
    </div>
    </section>
    
    <div class="container">
    <div class="hiddenscroll" id="data"></div>
    <section class="storyrow">
    <center><h2 class="hideme">The Data</h2></center>
    <section>
    <h3>Collecting the Data</h3>
    <p class="viscontent800">Before we decided on the specific data we were going to use, we first decided to download important financial metrics and begin exploring to determine how we would approach finding relevant financial data for our neural network. To collect data used the services of <a href="http://www.quandl.com/">Quandl</a>, an API service which allowed us to explore and download relevant financial data.</p>
    <p class="viscontent800">In the end, we downloaded 683 weeks of data which ranged from Sept. 1999 to Oct. 2012, a range by finding the chronological intersection of all our datasets). Before we were able to <i>use</i> the data, we had to perform a number of steps to process the data.</p>
    <a href = "wwww.quandl.com" id="quandlimg"><img src ="images/quandl.PNG" /></a>
    </section>
    <section style="position:relative;top:-30px;">
    <h3>Processing Data</h3>
    <p class="viscontent">Quandl allowed us to download financial data easily; however, our work was just beginning. The data retrieved from Quandl was not ready for use (for example, the individual dataframes were inconsistent with regards to time and range). We decided to use weekly data that extended at least ten years back and coded the following data procedures. All function names are in <b>bold</b> and can be found in our <a href="process.html">processbook.</a></p>
    <div class="twopanel viscontent">
        <div class="halfrow" style="border-right:1px solid #aaa;"><center><h4>Step 1: Cleaning & Combining</h4></center>
             <ul>
                <li>Downloading the data: we use <b>get_quandl_data</b> to download the dataset.
                </li>
                <li>We use <b>collapser</b> to perform linear interpolation on all datasets on a daily level then adjust the time scale to weekly. 
                </li>
                <li>We use <b>merger</b> to combine all the datasets into a single dataframe,and find the intersection dates of all datasets.</li>
            </ul>
            </div>
        <div class="halfrow"><center>
            <h4>Step 2: Calculation & Interpolation</h4></center>
            <ul>
                <li>With the dataframe, we use <b>percentageDifference</b> to append columns which are the week-over-week percentage differences of our datasets.
                </li>
                <li>We use <b>biner</b> to create the outputs for the training dataframe which return a '-1' or '1' based on the value of the percentage change in the s and p 500.
                </li>
                <li>Next, we use <b>advance_cols</b> to shift the output column by one date to create the 'correct' column by which we score our neural network's predictions.</li>
            </ul>
        </div>
    </div>
   </section>
    <h3 style="clear:both;">Exploring Data</h3>
 <div style="text-align: center;
padding: 10px;
border: 1px solid #EEE;
width: 520px;
margin: 0px auto;">
    <a href="images/fullscreen/p1.PNG" rel="prettyPhoto[pp_gal]" title="Summary metrics of our S&P data."><img src="images/thumbnails/t1.PNG" width="70" height="70" /></a>

<a href="images/fullscreen/p2.PNG" rel="prettyPhoto[pp_gal]"  title="A histogram of S&P returns."><img src="images/thumbnails/t2.PNG" width="70" height="70" /></a>

<a href="images/fullscreen/p3.png" rel="prettyPhoto[pp_gal]" title="A sample scatterplot of GDP vs S&P, one of many scatterplots."><img src="images/thumbnails/t3.PNG" width="70" height="70" /></a>

<a href="images/fullscreen/p4.PNG" rel="prettyPhoto[pp_gal]" title="An analysis of collinearity in our data."><img src="images/thumbnails/t4.PNG" width="70" height="70" /></a>

<a href="images/fullscreen/p5.PNG" rel="prettyPhoto[pp_gal]"  title="A color-coded display of S&P returns based on direction change (green is positive)."><img src="images/thumbnails/t5.PNG" width="70" height="70" /></a>
<a href="images/fullscreen/p6.png" rel="prettyPhoto[pp_gal]"  title="A color-coded display of S&P returns based similar direction (green is where all three indicies move in the same direction)."><img src="images/thumbnails/t6.PNG" width="70" height="70" /></a>
</div>
    <p class="viscontent">Having cleaned our data, we began exploring and visualizing all of our variables.  In our <a href="process.html">processbook</a>, we used <b>scatter_plot</b> and <b>colin</b> to plot and find the collinearity of our datasets, in addition to calculating over 10 metrics including average weekly change, extrema, standard deviation, etc.  A sample of our work is displayed in the interactive thumbnails above, with all our metrics and plots in the <a href="process.html">processbook</a>.  Later on, we also adapted a visual matrix plot with a selection of our exploratory data using d3.js, which is displayed below.
    
    <h3>Visualizing the Data - Webpage</h3>
    </p>The data was also visualized using for web interfaces (on this page). The neural network visualization was constructed from scratch using d3.js, and is one of the first web-based visualizations of neural networks to encode weights in edge color and width. Creating this surpisingly complex visualization involved navigation of large, sparse, three-dimensional arrays in javascript.  The matrix visualization was adapted from Mike Bostock's own code <a href="http://bl.ocks.org/mbostock/4063663">here.</a>  The visualization of Quandl codes was also constructed from scratch using d3.js.
    
    <h3 style="margin-top:40px;border-top:1px solid #ddd;padding-top:10px;">Visualizing the Data - Weekly Economic Data from 1999-2012</h3>   
    <p class="viscontent">The following displays weekly data which is chromatically coded based on S&P percentage returns: positive(green) means a weekly return of more than 1%, neutral (grey) is between -1% and 1%, and negative points (blue) represent weekly returns of worse than negative 1%. By holding down the mouse over the visualization, you can 'select' points to be highlighted across the matrix.</p> 
    <div class="forms">
    <form class="irisform">
    <p>First dataset:</p>
    <select id="trait1" onchange="selectData()" class="selectpicker"><optgroup label="Dataset Values">
    <option>cpi</option><option selected="selected">house sales</option><option>gas price</option><option>gold price</option><option>usd to pound</option><option>usd to euro</option><option>unemployment</option><option>s and p 500</option><option>volatility</option></optgroup><optgroup label="Week-over-Week Percent Change"><option>cpi pdiff</option><option>house sales pdiff</option><option>gas price pdiff</option><option>gold price pdiff</option><option>usd to pound pdiff</option><option>usd to euro pdiff</option><option>unemployment pdiff</option><option>s and p 500 pdiff</option><option>volatility pdiff</option></optgroup>
</select></form>
    <form class="irisform" style="margin-top: 80px;">
    <p>Second dataset:</p>
    <select id="trait2" onchange="selectData()" class="selectpicker"><optgroup label="Dataset Values">
    <option>cpi</option><option >house sales</option><option selected="selected">gas price</option><option>gold price</option><option>usd to pound</option><option>usd to euro</option><option>unemployment</option><option>s and p 500</option><option>volatility</option></optgroup><optgroup label="Week-over-Week Percent Change"><option>cpi pdiff</option><option>house sales pdiff</option><option>gas price pdiff</option><option>gold price pdiff</option><option>usd to pound pdiff</option><option>usd to euro pdiff</option><option>unemployment pdiff</option><option>s and p 500 pdiff</option><option>volatility pdiff</option></optgroup>
</select></form>
    <form class="irisform"  style="margin-top: 160px;">
    <p>Third dataset:</p>
    <select id="trait3" onchange="selectData()" class="selectpicker"><optgroup label="Dataset Values">
    <option>cpi</option><option selected="selected">house sales</option><option>gas price</option><option selected="selected">gold price</option><option>usd to pound</option><option>usd to euro</option><option>unemployment</option><option>s and p 500</option><option>volatility</option></optgroup><optgroup label="Week-over-Week Percent Change"><option>cpi pdiff</option><option>house sales pdiff</option><option>gas price pdiff</option><option>gold price pdiff</option><option>usd to pound pdiff</option><option>usd to euro pdiff</option><option>unemployment pdiff</option><option>s and p 500 pdiff</option><option>volatility pdiff</option></optgroup>
</select></form>
    <form class="irisform"  style="margin-top: 240px;">
    <p>Fourth dataset:</p>
    <select id="trait4" onchange="selectData()" class="selectpicker"><optgroup label="Dataset Values">
    <option>cpi</option><option selected="selected">house sales</option><option>gas price</option><option>gold price</option><option>usd to pound</option><option selected="selected">usd to euro</option><option>unemployment</option><option>s and p 500</option><option>volatility</option></optgroup><optgroup label="Week-over-Week Percent Change"><option>cpi pdiff</option><option>house sales pdiff</option><option>gas price pdiff</option><option>gold price pdiff</option><option>usd to pound pdiff</option><option>usd to euro pdiff</option><option>unemployment pdiff</option><option>s and p 500 pdiff</option><option>volatility pdiff</option></optgroup>
</select></form>
    </div>
        <div id="charts">
        </div>
        
    </section>
    
    <div class="hiddenscroll" id="model"></div>
     <section class="storyrow">
    <center><h2>The Model</h2></center>
   	<h3>Narrowing Down the Data</h3>
    <p>Up to this point, we relied primarily on our own intuition, prior work, and common sense for thinking of variables to include (we picked the ones you see on the news).  With Quandl, though, we could try a different tact: run every relevant dataset from Quandl has access to (total, 7000+!).  We applied a multinomial Naive Bayes classifier (see section on the Naive Bayesian model below) to the datasets to determine which of the 7000 datasets had predictive power, ending up with only 18 datasets left.   The following visualiation shows all the datasets we tested, and the datasets that we narrowed it down until.  All Quandl dataset codes are linked and interactive. (click on them!)</p>
    <center class="setbuttons"><div class="btn btn-primary" id="run_sets1">Step 1: Get Datasets List</div><div class="btn btn-primary" id="run_sets2" style="margin-left:50px;">Step 2: Narrow Down the Data</div></center>
    </section>
    </div> <!-- end container -->
    
    <div class="usedsets_container">
    <div class="rowaccent" id = "sets">
    	<center style="margin-top:30px;"><h3>Press the 'Get Datasets List' button to begin.</h3></center>
    </div>
    <div class="usedsets_info">
        <h3>The Final Data</h3>
        <p>Using the naive Bayesian classifier to find which of the 7000+ Quandl variables are available and have individual predictive power of the stock market, we found 18 variables. </p>
        <p>To address the issue of collinearity, we eliminate state-specific variables and combine these variables which we believe will provide some insight into the stock market. Our final input set of datasets can be seen as the labels in the input neurons in our neural network visualization from above.
        </div>
    </div>    
    <div class="container" style="position:relative;top:-350px;">
     <h3>The Bayesian Classifier</h3>
    <p class = "viscontent">As mentioned earlier, we wanted to find out which variables to include in our neural network, so we ran a naive Bayesian classifier to evaluating each potential variable.   Our naive Bayesian approach was split into three distinct stages: </p><ul><li>Stage 1: Using <b>NB_Add_Drop</b>, we iterated through the lists of datasets in arbitrary order and kept variables that improved the accuracy of the model (which accumulated all previous variables that had been tested).</li>
    <li>Stage 2: Using <b>NB_All_Vars</b>, we tested the variables with model of the S&P and the particular dataset, which addressed the problem of first-mover bias.  In stage 1 the first few variables that the model tries will get included, while later variables affect the model less (and are thus less likely to be included). This stage is more fair way to test each variable, giving each one a chance to succeed. <strong>It is with the result of this stage we construct a input dataset for our neural network.</strong></li>
    <li>Stage 3: Using <b>NB_Add_Drop_Top</b>, we apply the methodology of stage 1 only on the results of stage 2. Of the twenty top predictive variables, we add them together in arbitrary order, keeping those that improve the model until we have a final <b> predictive Bayesian classifier model.</b>
    </li></ul>    
    
   	<h3>Training and Testing the Neural Network</h3>
    <p class = "viscontent">After exploring and narrowing our data sets, we trained our neural network through the use of <a href="http://en.wikipedia.org/wiki/Backpropagation">backprogatation</a>, essentially showing the model many examples and letting it find the patterns between them. This is useful for feedforward networks like the MLP neural net we are using.  Because this is a supervised learning method, we have to specify our specific input set, which we do with data from 1999-2010 (we reserve 2011 and 2012 for testing).  The backpropagation training took anywhere from 5 minutes to up to 45 minutes (with an upper limit of infinity).  Of the many parameters we could have experimented with, we note the following:</p>
    <ul>
    <li><b>Hidden layers:</b> We decided to choose to only have <b>one</b> hidden layer because of computational simplicity (both in terms of algorithm implementation and computation time).  In addition, existing literature (see <a href="http://www.eecs.harvard.edu/~parkes/cs286r/spring08/cours.pdf">this presentation</a>) suggests that more than one hidden layer is unnecessary.</li>
    <li><b>Hidden neurons:</b> without enough hidden neurons, our model would predict increases too many times (and overlook decreases).  This was solved by incorporating more hidden neurons, but we hit an upper bound with regards to reasonable computational time. Thus, we settled for 14 input neurons.</li>
    <li><b># Input neurons:</b> this was decided by the dataset we came up with, which totaled 20 datasets / input neurons. </li>
    <li><b># Output neurons:</b> existing literature (again,<a href="http://www.eecs.harvard.edu/~parkes/cs286r/spring08/cours.pdf">this presentation</a>), suggests that having only one output is optimal.  Thus, we chose to have one output neuron (range -1 to 1).
    <li><b>Learning rate / epochs:</b> the number of training epochs to run (and corresponding learning rate) was largely determined by trial and error.  In addition, we constructed errors plots for intuition (see below). By doing so, approximate an 'optimal' learning rate and epoch time at 500 epochs at a learning rate of .0001.</li> We observed that with too high of a learning rate or if we used too many epochs the output would be extreme (nearly all preidictions were increases). Too few epochs or too low of a learning rate caused mostly predictions of decreases. The challenge was to balance the learning rate and number of epochs so that the network could be sufficiently trained, without overtraining and becoming useless. 
    </ul> 
        <p>The following is the mean-squared error (MSE) of the training and testing sets by epoch (iteration) of training the backpropagation algorithm.  As you can see, error decreases as the network is trained for both datasets until a certain point.</p>
    <div class="errorimg">
    	<img src="images/error.PNG"/>
    </div>
    
    <div class="hiddenscroll" id="results"></div>
    <section class="storyrow">
    <center><h2>Results</h2></center>
   	<p>The following is an overview of the results for both networks are shown below, with more detailed results in our <a href="process.html">processbook</a>. </p>
    
    <h3>Prediction Probabilities and Errors</h3>
    <center>
    <div class="rowcontainer">
        <div class="halfrow" style="border-right:1px solid #eee">
        	<center><h3>The Neural Network</h3></center>
            <p class="note">*This result varies per trial on random weight initialization for the neural network.</p>
            <div class="infobox">
                <h4>Prediction accuracy <b>overall:</b></h4>
                <h1>63.64%</h1>
            </div>
            <div class="infobox">
                <h4>Accuracy for <b>increases</b>:</h4>
                <h1>79.37%</h1>
            </div>
           <div class="infobox">
                <h4>Accuracy for <b>decreases</b>:</h4>
                <h1>36.11%</h1>
            </div>
        </div>
        <div class="halfrow">
            <center><h3>The Bayesian Classifier</h3></center>
            <p class="note">*This result is consistent across trials and remains constant.</p>
            <div class="infobox">
                <h4>Prediction accuracy <b>overall:</b></h4>
                <h1>61.62%</h1>
            </div>
            <div class="infobox">
                <h4>Accuracy for <b>increases</b>:</h4>
                <h1>79.69%</h1>
            </div>
           <div class="infobox">
                <h4>Accuracy for <b>decreases</b>:</h4>
                <h1>28.57%</h1>
            </div>
        </div>
    </div>
    </center>
    <div class="halfrow">
    <h3 style="clear:both;">Neural Network Results</h3>
    <p>The Neural Network results were somewhat dependent on the network initialization (weights are randomly generated, at first, but generally were surprisingly positive. The network performed well most of the time, but tends to be better at predicting increases than decreases.</p>    <div class="errorimg">
    	<img src="images/results2.jpg"/>
    </div>
    </div>
    <div class="halfrow">
    <h3 style="clear:both;">Bayesian Classifier Results</h3>
    <p>The Bayesian Classifier performed surprisingly well as well, outperforming the market nearly as well as the neural network did (though sharing the same flaw of overpredicting increases, even more so than the neural network). </p>
       <div class="errorimg">
    	<img src="images/results.png"/>
    </div>
    </div>
    <p style="clear:both">The above figures show movement in the S&P and accompanying predictions: green rectangles represent accurate predictions, red rectangles represent inaccurate predictions, and the black figures are the percent changes in the S&P. </p>

    
    </section>
    
    <div class="hiddenscroll" id="applications"></div>
    <section class="storyrow">
    <center><h2>Applications</h2></center>
    <p>Given the results of the model, we wanted to see how well it would perform in the wild. To do so, we formulated a simplistic trading strategy to run with our weekly predictions:</p>
    <center>
    <div class="infobox" style="width:700px;"><h3>How We Traded</h3>
    <p style="text-align:left;">If we predict an increase, than we invest all of our money in the S&P 500 index.  If we predict a decrease, we take all our money out as cash and hold it until we predict an increase.  This simulation is run every week, running the model on Monday then placing the buy/sell order.</p></div>
    </center>
    <h3 style="margin-top:20px;">Trading Strategy Results</h3>
    <p>We ran the trading strategy over 98 weeks (approx two years) starting in 2011. Our model resulted in a trading strategy that created a return of <b>25.09%</b> while the S&P index increased <b>22.69%</b> over the same period.</p>
    <p>The first graph shows the percent return of a hypothetical portfolio using our strategy and one which does not.  The second graph shows the relative percentage difference with the index over time. <b>Our model performed 2.39% better than the market.</b></p>
        <div class="resultimgs">
        <center>
        <img src="images/trading1.png" style="margin-left:100px;" />
        <img src="images/trading2.png"/>
        </center>
        </div>
    </section>
    
    <div class="hiddenscroll" id="conclusions"></div>
    
    <section class="storyrow" style="border-top:none;position: relative;top:60px;">
    <center><h2 style="clear:both;">The Conclusion</h2></center>
    <p>
    We were successful in exploring the use of Artificial Neural Networks for analyzing and predicting financial time series data. By taking a 'shotgun' programmatic approach to data selection, we not only found a set of input data that provides reasonable accuracy (beating the market) but also added to the usefulness of neural networks by developing a more data-driven approach to input selection. Both models were cross-validated across parameters and were successful in providing insight into stock market direction over the weekly time scale. Moreover, we were able to construct novel and dazzling visualizations of our neural network weights using d3.  Finally, we were able to test our neural network with an effective trading strategy that actually outperformed the S&P index.
    </p>
    </section>
    
</div>

<div class="footer">
<div id="footerbody">
			<span>Data Source: Quandl (various, incl. U.S. Federal Reserve, see links)</span><br>
			<span>Powered by <a href= "http://jquery.com/">jQuery</a>, <a href ="http://www.no-margin-for-errors.com/projects/prettyphoto-jquery-lightbox-clone/">PrettyPhoto</a>, <a href = "http://d3js.org/">d3.js</a></span><br>
			<span>Matrix brushing visualization adapted from <a href="http://bl.ocks.org/mbostock/4063663">here.</a> </span>
		</div>
        
</div>
<div class = "infopanel" style="top:1440px;">
    <p>By the end of the project, we scraped over <strong>7000+ datasets!</strong></p>
</p>
</div>

<div class = "infopanel" style="margin-top:1100px">
    <p>You can make more than 3000 distinct matrices with this visualization!</p>
</p>
</div>

<div class = "infopanel" style="margin-top:900px">
    <p>The neural network visualization above is the first of its kind online to use actual weights in encoding edge width and color!</p>
</p>
</div>

<!-- Introductory Modal -->
<div class="modal fade" id = "introModal">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
        <center><h1 class="modal-title">A Neural Network Application: Predicting the Stock Market</h1></center>
      </div>
      <div class="modal-body">
        <p>We want to beat the market using financial indicators to predict a simple question: will the stock market go up or down next week? Close and View to explore our data and see our results. Check out the IPython Processbook for a more in-depth view of how we selected our data and ran our prediction engine. Or, watch the screencast if you'd rather have us walk you through it.</p>
      </div>
      <div class="modal-footer">
      <center>
      <a href="http://www.youtube.com/watch?v=U0nvXHh7o-w" rel="prettyPhoto"><button type="button" class="btn btn-primary">Screencast</button></a>
      <button type="button" class="btn btn-primary">IPython Processbook</button>
       <button type="button" class="btn btn-default" data-dismiss="modal">Close & View Project</button>
		</center>
      </div>
    </div><!-- /.modal-content -->
  </div><!-- /.modal-dialog -->
</div><!-- /.modal -->
<script type="text/javascript">
	$('#introModal').modal();
	$("a[rel^='prettyPhoto']").prettyPhoto();
    console.log("premature load");
    </script>
    
</body>
</html>
